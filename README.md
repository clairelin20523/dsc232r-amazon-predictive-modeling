### Get Total Number of Observations
```df.count()```
### Get Columns
```df.columns```
### Get Number of Columns
```len(columns)```
### Get Number of Missing Values for Each Column
```​​for i in range(num_cols):
    missing = df.filter(df[columns[i]].isNull()).count()
    print("'%s' column has %d missing values" % (columns[i], missing))```
### Look at Column Types
```data_types = df.dtypes
for i in range(num_cols):
    print("'%s' column is of type '%s'" % (data_types[i][0], data_types[i][1]))```

# dsc232r-amazon-predictive-modeling

Download the dataset and save unzipped files in a folder named 'amazon_data':
https://www.kaggle.com/datasets/cynthiarempel/amazon-us-customer-reviews-dataset

In processing our dataset, proper data preparation is crucial to ensure the quality and effectiveness of our analysis. Here are the specific steps we have taken:

### Missing Value Handling
For the essential fields `review_body` and `review_date`, which are critical for our analysis due to their relevance to the review content and timing, we will remove any rows with missing values in these columns. The absence of this information renders the row useless for trend analysis. For missing values in the `product_category` field, since adjacent entries usually belong to the same category, we will employ a forward-fill method to maintain data continuity. Missing values in other fields will be filled based on their data type using the median or mode.

### Data Filtering and Simplification
Given the low proportion of Vine program reviews (only 2,982 out of 523,269), which is too minor to significantly impact our analysis, we have decided to drop this column. Similarly, since all data comes from the US market (the `marketplace` column is always 'US'), this column is redundant and will be removed to save storage space and computational resources.

### Data Time Range Adjustment
Observing that the volume of data increases over the years, likely reflecting the growing base of Amazon users, we will discard data from before 2005. This approach focuses the model training on more representative and relevant data, enhancing the accuracy of predicting future trends.

### Data Encoding and Text Processing
Text category labels, such as `product_category`, will be converted into numerical codes to reduce the complexity of model processing and improve computational efficiency. We will also tokenize the review texts, which are essential for extracting useful information and patterns for text analysis.

### Anomaly Data Handling
We will identify and remove reviews likely generated by automated scripts, as such data typically feature monotonous sentiments and high repetition rates, which could affect the model's accuracy and generalizability. Arima model also has a seasonality component that helps to identify residuals.

### Feature Standardization
We will standardize or normalize numerical features, especially when using distance-based models, to optimize model performance and predictive capabilities.

### Dataset Splitting
Data will be split into training and testing sets according to the time order, with the most recent data used as the test set. This setup simulates real-world predictions of product trends, ensuring that the model performs well on unseen data.

Through these meticulous data preprocessing steps, our dataset will be cleaner, more effective, and ready to build an accurate predictive model.
