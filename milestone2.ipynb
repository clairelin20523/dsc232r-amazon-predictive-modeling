{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Amazon Review Data : Data Exploration\n",
    "\n",
    "### Setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, pickle, glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "\t.config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config('spark.executor.instances', 7) \\\n",
    "\t.appName(\"Amazon Reviews\") \\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Read Data\n",
    "\n",
    "### Get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = sc.read.csv(\"amazon_reviews_us_Gift_Card_v1_00.tsv\", sep=\"\\t\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "############## change to get to my folder: uci150/clin6/amazon_data ###################\n",
    "#######################################################################################\n",
    "path = \"amazon_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    \"\"\"\n",
    "    Method that loads data file as df\n",
    "    Takes in 1 parameter: path\n",
    "    \"\"\"\n",
    "    return sc.read.csv(path, sep = \"\\t\", header = True, inferSchema = True)\n",
    "\n",
    "def get_path(file):\n",
    "    \"\"\"\n",
    "    Method to create path\n",
    "    Takes in 1 parameter: file name\n",
    "    \"\"\"\n",
    "    return \"amazon_data/%s\" % file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'amazon_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7f916e5ca944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'amazon_data'"
     ]
    }
   ],
   "source": [
    "dir = os.listdir(path)\n",
    "files = [f for f in dir if os.path.isfile(os.path.join(path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df(files):\n",
    "    \"\"\"\n",
    "    Method that combines files into 1 big df\n",
    "    Takes in 1 parameter: list of file names\n",
    "    \"\"\"\n",
    "    df = read(get_path(files[0]))\n",
    "    n = len(files)\n",
    "    for i in range(1, n):\n",
    "        data = read(get_path(files[i]))\n",
    "        df = df.union(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(files)\n",
    "sqlContext.registerDataFrameAsTable(df, \"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration Start\n",
    "* Perform the data exploration step (i.e. evaluate your data, # of observations, details about your data distributions, scales, missing data, column descriptions) Note: For image data you can still describe your data by the number of classes, # of images, plot example classes of the image, size of images, are sizes uniform? Do they need to be cropped? normalized? etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|        review_date|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
      "|         US|   24371595|R27ZP1F1CD0C3Y|B004LLIL5A|     346014806|Amazon eGift Card...|       Gift Card|          5|            0|          0|   N|                Y|          Five Stars|Great birthday gi...|2015-08-31 00:00:00|\n",
      "|         US|   42489718| RJ7RSBCHUDNNE|B004LLIKVU|     473048287|Amazon.com eGift ...|       Gift Card|          5|            0|          0|   N|                Y|Gift card for the...|It's an Amazon gi...|2015-08-31 00:00:00|\n",
      "|         US|     861463|R1HVYBSKLQJI5S|B00IX1I3G6|     926539283|Amazon.com Gift C...|       Gift Card|          5|            0|          0|   N|                Y|          Five Stars|                Good|2015-08-31 00:00:00|\n",
      "|         US|   25283295|R2HAXF0IIYQBIR|B00IX1I3G6|     926539283|Amazon.com Gift C...|       Gift Card|          1|            0|          0|   N|                Y|            One Star|                Fair|2015-08-31 00:00:00|\n",
      "|         US|     397970| RNYLPX611NB7Q|B005ESMGV4|     379368939|Amazon.com Gift C...|       Gift Card|          5|            0|          0|   N|                Y|          Five Stars|I can't believe h...|2015-08-31 00:00:00|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Show how our data looks like\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a total of 149086 observations\n"
     ]
    }
   ],
   "source": [
    "### Total number of observations\n",
    "num_rows = df.count()\n",
    "print('There is a total of %d observations' % num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n"
     ]
    }
   ],
   "source": [
    "### Get columns\n",
    "columns = df.columns\n",
    "print('Columns:')\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a total of 15 columns\n"
     ]
    }
   ],
   "source": [
    "### Get number of columns\n",
    "num_cols = len(columns)\n",
    "print('There is a total of %d columns' % num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### removing repetitive/unnecessary information\n",
    "df = df.select([col for col in df.columns if col not in ['marketplace', 'vine']])\n",
    "\n",
    "### var after updated df\n",
    "columns = df.columns\n",
    "num_cols = len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'customer_id' column has 0 missing values\n",
      "'review_id' column has 0 missing values\n",
      "'product_id' column has 0 missing values\n",
      "'product_parent' column has 0 missing values\n",
      "'product_title' column has 0 missing values\n",
      "'product_category' column has 0 missing values\n",
      "'star_rating' column has 0 missing values\n",
      "'helpful_votes' column has 0 missing values\n",
      "'total_votes' column has 0 missing values\n",
      "'verified_purchase' column has 0 missing values\n",
      "'review_headline' column has 0 missing values\n",
      "'review_body' column has 6 missing values\n",
      "'review_date' column has 5 missing values\n"
     ]
    }
   ],
   "source": [
    "### Get number of missing values for each column\n",
    "for i in range(num_cols):\n",
    "    missing = df.filter(df[columns[i]].isNull()).count()\n",
    "    print(\"'%s' column has %d missing values\" % (columns[i], missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'customer_id' column is of type 'int'\n",
      "'review_id' column is of type 'string'\n",
      "'product_id' column is of type 'string'\n",
      "'product_parent' column is of type 'int'\n",
      "'product_title' column is of type 'string'\n",
      "'product_category' column is of type 'string'\n",
      "'star_rating' column is of type 'int'\n",
      "'helpful_votes' column is of type 'int'\n",
      "'total_votes' column is of type 'int'\n",
      "'verified_purchase' column is of type 'string'\n",
      "'review_headline' column is of type 'string'\n",
      "'review_body' column is of type 'string'\n",
      "'review_date' column is of type 'timestamp'\n"
     ]
    }
   ],
   "source": [
    "data_types = df.dtypes\n",
    "for i in range(num_cols):\n",
    "    print(\"'%s' column is of type '%s'\" % (data_types[i][0], data_types[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------+----------+--------------------+--------------------+----------------+------------------+-------------------+------------------+-----------------+--------------------+--------------------+\n",
      "|summary|         customer_id|     review_id|product_id|      product_parent|       product_title|product_category|       star_rating|      helpful_votes|       total_votes|verified_purchase|     review_headline|         review_body|\n",
      "+-------+--------------------+--------------+----------+--------------------+--------------------+----------------+------------------+-------------------+------------------+-----------------+--------------------+--------------------+\n",
      "|  count|              149086|        149086|    149086|              149086|              149086|          149086|            149086|             149086|            149086|           149086|              149086|              149080|\n",
      "|   mean| 2.628253678963149E7|          null|      null| 5.406851020515139E8|                null|            null| 4.731363105858364|0.39660330279167727|0.4894490428343372|             null|3.835337059232766...|3.7860952672727275E8|\n",
      "| stddev|1.5874673170314249E7|          null|      null|2.6613081670640522E8|                null|            null|0.8293064689866771| 20.649231330376796|22.766276885000547|             null|9.394618786622647E13|1.2557057010822508E9|\n",
      "|    min|               10637|R10015C83OW4BX|B0002CZPPG|             1100879|    Amazon Allowance|       Gift Card|                 1|                  0|                 0|                N|              \u001a\u001a\u001a\u001a\u001a\u001a|\u001a\u001a\u001a   This card a...|\n",
      "|    max|            53096482| RZZYX2X2MBRUL|BT00DDZD6G|           999274173|Graduation Name C...|       Gift Card|                 5|               5987|              6323|                Y|                  😋|            😴😴😹😹|\n",
      "+-------+--------------------+--------------+----------+--------------------+--------------------+----------------+------------------+-------------------+------------------+-----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Show data distributions\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+-----------------+---------------+-----------+-----------+-----+\n",
      "|customer_id|review_id|product_id|product_parent|product_title|product_category|star_rating|helpful_votes|total_votes|verified_purchase|review_headline|review_body|review_date|count|\n",
      "+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+-----------------+---------------+-----------+-----------+-----+\n",
      "+-----------+---------+----------+--------------+-------------+----------------+-----------+-------------+-----------+-----------------+---------------+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Check for duplicates\n",
    "duplicate_rows = df.groupBy(columns).count().where('count > 1')\n",
    "duplicate_rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot\n",
    "reference:\n",
    "https://plotly.com/python/v3/apache-spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Import libraries to plot\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "py.init_notebook_mode(connected = True)\n",
    "\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot(title, data):\n",
    "    \"\"\"\n",
    "    Helper method to plot\n",
    "    Takes in two paramaters: title, data\n",
    "    \"\"\"\n",
    "    layout = go.Layout(title = title)                       # Create layout\n",
    "    fig = go.Figure(data = data, layout = layout)           # Create figure\n",
    "    iplot(fig)                                              # Plot figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram(df, i):\n",
    "    \"\"\"\n",
    "    Method to plot histogram of column\n",
    "    Takes in two parameters: df, column index\n",
    "    \"\"\"\n",
    "    column_name = columns[i]                                # Get column name and data\n",
    "    data = [go.Histogram(x = df.select(col(column_name)).rdd.flatMap(lambda x: x).collect())]\n",
    "    title = 'Histogram of %s' % columns[i]                  # Set title\n",
    "    plot(title, data)                                       # Use helper method to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_piechart(df, i):\n",
    "    \"\"\"\n",
    "    Method to plot pie chart of column\n",
    "    Takes in two parameters: df, column index\n",
    "    \"\"\"\n",
    "    column_name = columns[i]                                                      # Get column name\n",
    "    counts = df.select(col(column_name)).groupBy(column_name).count().toPandas()  # Get counts\n",
    "    data = [go.Pie(labels = counts[column_name], values = counts['count'])]       # Get data\n",
    "    title = 'Pie Chart of %s' % columns[i]                                        # Set title\n",
    "    plot(title, data)                                       # Use helper method to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_barplot(df, i):\n",
    "    \"\"\"\n",
    "    Method to plot bar plot of column\n",
    "    Takes in one parameter: column index\n",
    "    \"\"\"\n",
    "    column_name = columns[i]                                                      # Get column name \n",
    "    counts = df.select(col(column_name)).groupBy(column_name).count().toPandas()  # Get counts\n",
    "    data = [go.Bar(x = counts[column_name], y = counts['count'])]                 # Get data    \n",
    "    title = 'Bar Plot of %s' % columns[i]                                         # Set title\n",
    "    plot(title, data)                                       # Use helper method to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_boxplot(df, i):\n",
    "    \"\"\"\n",
    "    Method to plot box plot of column\n",
    "    Takes in two parameters: df, column index\n",
    "    \"\"\"\n",
    "    column_name = columns[i]                                # Get column name and data\n",
    "    data = [go.Box(y = df.select(column_name).rdd.flatMap(lambda x: x).collect())]\n",
    "    title = 'Box plot of %s' % column_name                  # Set title\n",
    "    plot(title, data)                                       # Use helper method to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_boxplot_no_outliers(df, i):\n",
    "    \"\"\"\n",
    "    Method to plot box plot of column after removing outliers\n",
    "    Takes in two parameters: df, column index\n",
    "    \"\"\"\n",
    "    column_name = columns[i]                                             # Get column name\n",
    "    quartiles = df.approxQuantile(column_name, [0.25, 0.75], 0.01)       # Calculate quartiles\n",
    "    q1 = quartiles[0]\n",
    "    q3 = quartiles[1]\n",
    "    iqr = q3 - q1                                                        # Calculate iqr\n",
    "    lower_bound = q1 - 1.5 * iqr                                         # Define bounds\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Remove outliers\n",
    "    column_data_no_outliers = df.filter((col(column_name) >= lower_bound) & (col(column_name) <= upper_bound))\n",
    "    \n",
    "    # Filter data\n",
    "    data = [go.Box(y = column_data_no_outliers.select(column_name).rdd.flatMap(lambda x: x).collect())]\n",
    "    \n",
    "    title = 'Box plot of %s (Outliers Removed)' % columns[i]             # Set title\n",
    "    plot(title, data)                                                    # Use helper method to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 82.0 failed 1 times, most recent failure: Lost task 2.0 in stage 82.0 (TID 741, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-9641e1dfb46e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### CHANGE COLUMN INDEX TO PLOT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-e9e2f1353dad>\u001b[0m in \u001b[0;36mplot_histogram\u001b[0;34m(df, i)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m      6\u001b[0m     \u001b[0mcolumn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m                                \u001b[0;31m# Get column name and data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Histogram of %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m                  \u001b[0;31m# Set title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m                                       \u001b[0;31m# Use helper method to plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 82.0 failed 1 times, most recent failure: Lost task 2.0 in stage 82.0 (TID 741, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "### CHANGE COLUMN INDEX TO PLOT\n",
    "i = 7\n",
    "plot_histogram(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are clearly noticeable high outliers, which conceal the visibility of the distribution\n",
    "* Lets look at a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 76.0 failed 1 times, most recent failure: Lost task 0.0 in stage 76.0 (TID 505, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-1098a08442c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### CHANGE COLUMN INDEX TO PLOT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_boxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-f6d11fabb0df>\u001b[0m in \u001b[0;36mplot_boxplot\u001b[0;34m(df, i)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m      6\u001b[0m     \u001b[0mcolumn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m                                \u001b[0;31m# Get column name and data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Box plot of %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcolumn_name\u001b[0m                  \u001b[0;31m# Set title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m                                       \u001b[0;31m# Use helper method to plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 76.0 failed 1 times, most recent failure: Lost task 0.0 in stage 76.0 (TID 505, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "### CHANGE COLUMN INDEX TO PLOT\n",
    "i = 7\n",
    "plot_boxplot(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Indeed we see many high outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 87.0 failed 1 times, most recent failure: Lost task 6.0 in stage 87.0 (TID 971, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-405d34823c04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### CHANGE COLUMN INDEX TO PLOT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_boxplot_no_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-7d19d0f5614c>\u001b[0m in \u001b[0;36mplot_boxplot_no_outliers\u001b[0;34m(df, i)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Filter data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_data_no_outliers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Box plot of %s (Outliers Removed)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m             \u001b[0;31m# Set title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 87.0 failed 1 times, most recent failure: Lost task 6.0 in stage 87.0 (TID 971, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "### CHANGE COLUMN INDEX TO PLOT\n",
    "i = 7\n",
    "plot_boxplot_no_outliers(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Outliers removed box plot, we see most reviews have no votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          1,
          3,
          5,
          4,
          2
         ],
         "y": [
          4793,
          3156,
          129709,
          9859,
          1569
         ]
        }
       ],
       "layout": {
        "title": "Bar Plot of star_rating"
       }
      },
      "text/html": [
       "<div id=\"3904c922-41f7-43f3-b55b-fc47bc47a993\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"3904c922-41f7-43f3-b55b-fc47bc47a993\", [{\"type\": \"bar\", \"x\": [1, 3, 5, 4, 2], \"y\": [4793, 3156, 129709, 9859, 1569]}], {\"title\": \"Bar Plot of star_rating\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"3904c922-41f7-43f3-b55b-fc47bc47a993\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"3904c922-41f7-43f3-b55b-fc47bc47a993\", [{\"type\": \"bar\", \"x\": [1, 3, 5, 4, 2], \"y\": [4793, 3156, 129709, 9859, 1569]}], {\"title\": \"Bar Plot of star_rating\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### CHANGE COLUMN INDEX TO PLOT\n",
    "i = 6\n",
    "plot_barplot(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Majority of ratings given are 5 stars\n",
    "* Second most given rating is 1 but still much less than 5\n",
    "* Increased ratings from 2 to 5 stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at category proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "labels": [
          1,
          3,
          5,
          4,
          2
         ],
         "type": "pie",
         "values": [
          4793,
          3156,
          129709,
          9859,
          1569
         ]
        }
       ],
       "layout": {
        "title": "Pie Chart of star_rating"
       }
      },
      "text/html": [
       "<div id=\"151ebdef-162a-4595-81e0-585a8fcaa49d\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"151ebdef-162a-4595-81e0-585a8fcaa49d\", [{\"type\": \"pie\", \"labels\": [1, 3, 5, 4, 2], \"values\": [4793, 3156, 129709, 9859, 1569]}], {\"title\": \"Pie Chart of star_rating\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"151ebdef-162a-4595-81e0-585a8fcaa49d\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"151ebdef-162a-4595-81e0-585a8fcaa49d\", [{\"type\": \"pie\", \"labels\": [1, 3, 5, 4, 2], \"values\": [4793, 3156, 129709, 9859, 1569]}], {\"title\": \"Pie Chart of star_rating\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### CHANGE COLUMN INDEX TO PLOT\n",
    "i = 6\n",
    "plot_piechart(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, substring\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_purchase_counts(df, df_name, target_year):\n",
    "    \"\"\"\n",
    "    Plot the purchase counts for a given DataFrame and year.\n",
    "\n",
    "    Args:\n",
    "    - df: The DataFrame containing the review data.\n",
    "    - target_year: The year for which purchase counts will be plotted.\n",
    "    - df_name: The name of the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include only rows from the specified year\n",
    "    df_year = df.filter(year(df['review_date']) == target_year)\n",
    "\n",
    "    # Extract the month from the 'review_date' column\n",
    "    df_year = df_year.withColumn('review_month', substring(df_year['review_date'], 6, 2))\n",
    "\n",
    "    # Count the number of purchases for each month\n",
    "    purchase_counts = df_year.groupby('review_month').count().orderBy('review_month')\n",
    "\n",
    "    # Plot the counts using a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(purchase_counts.toPandas()['review_month'], purchase_counts.toPandas()['count'])\n",
    "    plt.title(f'Purchase Counts for {df_name} Year {target_year}')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Purchases')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnQAAAGDCAYAAACiFo3zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzt3Xm8JFV99/HPV0AUFYdlguyDgFGMS8yIGPO4YUQhCk/ighuEB0MScYk7miiKQTHuJNGEKAq4oKIiAmpQ45JEEGSTRWVEkEGWUXYQEPg9f9S52ox36Vm6+9bM5/169et2narq/vW5zdwvp+pUpaqQJElSf91j0gVIkiRp1RjoJEmSes5AJ0mS1HMGOkmSpJ4z0EmSJPWcgU6SJKnnDHTSWirJx5L846Tr6KMkj0tyUZKbkuw1D+q5JMlTJl2HpMkx0EnzTPvj/KsWFq5qweu+k65rVJI8P8kZ7fNekeTLSf5kDO9bSXZYyd0PAf6lqu5bVcevhlo+luT21gc3Jvl+kies6uuuYA2bJflFkicu135kkmPH8P4PSfKlJMuSXNO+Bzsut81rk1yZ5PokH05yz4F1b09yXpI7kvzDcvs9JcldrX+nHi8Y9WeSxslAJ81Pz6iq+wKPAhYD/zDH9r8jybqrvarVLMmrgPcDbwc2A7YBPgjsOcm6hrAtcP7K7DjL7+Wf2u98Q+BDwOeTrLOS9a2wqroKeCXwH0nuDZBkV+DPgJetzveaoQ/uD3we+H2678LZwBcG9tkDeDXwJGC7tt2bB/b/MfAa4CszvO3PWgCfenxilT+INI8Y6KR5rKouB74M/AH87qG1JG9J8vH2fFEbddo/yc+Ab7T2P0nyv0muS3JZkr8ceIuNkpzURoVOS7L9wGt/oG1/Qxsx+j8D63Zuo2o3tFHE9w6s22Xg/c5ZfsRnYLv70410HVhVn6+qm6vq11X1pap6bdtm/STvT/Lz9nh/kvXbur9M8t/LveZvRt3aqNe/Tvf5kny77XJOG615bpJNk5zY6r4myXeS/M6/kUl+AjwQ+FLbd/0kWyQ5oe23JMlfLfc7Oi7Jx5PcAPzl8q85qLrb93wS2Jgu2JBk+yTfSPLLNor2iSQLltv10UkuSHJtko8muVfb97wkzxioZ732Gn84zXsfA/wIOKSFun8HXl5Vy9q+WyX5QhtF+2mSAwde97FJTm39d0WSw5Os19at2343L0myBPjhNO99alV9tKquqapfA+8DHtq+JwD7AkdU1YVVdQ3wj4N9WVUfq6qvADfN1r/SmspAJ81jSbYGdgfOWoHdngA8BNgtybZ0gfCfgYXAI+lGPqbsDbwV2AhYAhw6sO70tv3GdAHjs1MhAfgA8IGq2hDYHvhMq3dL4CS6P7Yb042YfC7JwmnqfCxwLwZGYabx98AurY5HADuzYqOV036+qnp8W/+INlrzabrRn6V0/bQZ8Ebgd+6NWFXbAz+jjaJW1W3AsW3fLYBnAW9P8uSB3fYEjgMWALOODLVRuX2AnwJXTTUD72iv/xBga+Aty+36AmA3ut/Hg/htPx0NvHBgu92BK6pqpu/U3wD/r32m86rq2FbXPYAT6b4XWwJ/Cry2jeIB3AG8AtgUeBzwNOCvl3vtZwKPBh42SxdMeTywtKqub8sPBc4ZWH8OsOVA4JvLFu1/Pi5O8p4kGwy5n9QLBjppfjo+yXXAfwPfojskOay3tNGuXwHPB75WVZ9qo1+/rKrBQPeFqvpeVd1BFzQeObWiqj7etr+jqt4DrE93mAvg18AOSTatqpuq6tTW/kLg5Ko6uaruqqpTgDPoQsTyNgF+0d57Ji8ADqmqq9so0VuBF61AX8z4+abxa2BzYNvWV9+pIW523UL344DXV9WtrX8/TBfKpny3qo5vffKrGV7qNe13fhPdYeg3VdWdAFW1pKpOqarbWj+8ly64D/qXqrqsjV4dCjyvtX8c2D3Jhm35RcAxM32eqlpKdyjzKcDfDqx6LLBhVb29qm6vqiXAR+hCM1V1elWd1r4vFwNHTFPj26vq2ln6AIAk2wCHA68aaL4vcP3A8tTz+832Ws35dP9DsDldEN0FeNcQ+0m9YaCT5qe9qmpBVW1bVS+Z6w/gci4beL418JNZtr1y4PktdH80AUjymiQXpjsB/Tq6c5w2bav3pxsF+mGS05P8WWvfFnh2O+x2XdvvT+j+kC7vl8Cmmf1cvy2ASweWL21tw5rx803jXXSjeP/ZRnEOGvI9tgCuqaobl6tzy4Hly5jbu6tqAbAB3XmT70rydPjNhIVjk1zeDtt+nN/+LqZ7j9/0U1X9HPgf4C/aYdqnM8coIV0Auraqrhho2xbYZrnf7euAB7QaH9wOb1/ZajxkjhqnleT3gP+kGwH+7MCqm+jOL5wy9Xyw36dVVVe0Q7V3VdVPgNfTjaRKawwDndQvN9P9wZ/ygGm2GRxVuozuENwKaefLvQ54DrBRCxrX0x36o6ouqqrnAb8HvBM4Lsl92vsd08Lo1OM+VXXYNG/zXeA2YLbLfvycLkhM2aa1wXJ9kWS6vhhaVd1YVa+uqgfSHRp81cDhxNn8HNg4yeBI0TbA5YMvvwJ1VFWdRxfC9mjNb2+v8bB2mPuFtN/FgK2Xe/+fDywf1fZ5Nt1o4WBtw7oMuGi53+39qmrq/Lx/B84Ddmg1vnmaGmfthySbAF8Djquqdy63emqUbcojgMsHDsmuiJqmNqnXDHRSv5wN7N1ObF/M3KMMnwCekuQ57cT0TZLMdthxyv3ozolaBqyb5M0MjI4keWGShVV1F3Bda76LbuToGUl2S7JOknsleWKSrZZ/g/aH+M3AvybZK8kG7XM9Pck/tc0+BfxDkoVJNm3bf7ytO4fupPlHtnP73jLE5xp0Fd3khqnP9GdJdkgSuvB6Z/tMs6qqy4D/Bd7RPu/D6UYwPz77njNL8mC6kc2pmbT3oxuhur6dp/jaaXY7sE1a2Jju3MNPD6w7nm7G9CvozqlbGd8Fbk/y6vY510nysCR/NFDj9cDNSR7C754/N6t2Ltx/At+oqunOkzwa+Ks2ErgR3TmCHxvYf732PbgH3Xf2Xu28P5I8qR0anzqc+w7giytSnzTfGeikfnkT3YjbtXTnk31yto2r6md056+9GriGLhA+YrZ9mq/SXf7hx3SH727l7ofLngacn+QmugkSe1fVr1q42ZNuQsGyts9rmeHfmnZu3qvo/jhPbf9SugAC3eSKM4BzgR8AZ7Y2qurHdIf1vgZcRHe+4Yp4C3BUO3z4HGDH9lo30YWXD1bVfw35Ws8DFtGNin0BOLiqvraC9bwu3azZm+mCzUfpRr2g+10/ii4wnUR3eY/lfbLtdzHdYfbfXDS6HbL/HN3lPqbbd07tPMTd6SamXAL8otU3FfRfTTcT9cbW/unffZVZPYvuM744d79e3NSh4xPpZr5+m+47eRHd73/KR4Ff0Y1CHtyeP7+tWwycmuQWuu/JmXSXaJHWGBninF9JUs+1UdYHVdUL59xYUu/M+wuPSpJWTTsMuz8rNkNYUo94yFWS1mDpLnJ8GfDlqvr2XNtL6icPuUqSJPWcI3SSJEk9N7JAl+TIJFcnOW+gbeMkpyS5qP3cqLUn3X3/liQ5N8mjBvbZt21/UZJ9B9r/KMkP2j6Ht0sNSJIkrXVGdsg1yePppv8fXVVTNxb/J7orqh/WrsK+UVW9PsnuwMvopsQ/hu4K4Y9pJ/KeQTflvIDvA39UVdcm+R7wcuA04GTg8Kr68lx1bbrpprVo0aLV/XElSZJWu+9///u/qKrp7od9NyOb5VpV306yaLnmPYEntudHAd+kuwXLnnTBr+iuFbQgyeZt21PavQlJcgrwtCTfpLun4Kmt/Wi6q83PGegWLVrEGWecsSofTZIkaSySXDr3VuM/h26zgXsDXgls1p5vyd0vWrq0tc3WvnSa9mklOSDJGUnOWLZs2ap9AkmSpHlmYpMi2mjcWKbYVtURVbW4qhYvXDjnqKUkSVKvjDvQXdUOpdJ+Xt3aL+fuN5beqrXN1r7VNO2SJElrnXEHuhPo7vVH+/nFgfZ92mzXXYDr26HZrwJPTbJRmxH7VOCrbd0NSXZps1v3wRstS5KktdTIJkUk+RTdpIZNkyylu1nyYcBnkuxPd3Pl57TNT6ab4boEuAXYD6CqrknyNuD0tt0hUxMkgJcAHwPuTTcZYs4JEZIkSWuite5OEYsXLy5nuUqSpD5I8v2qWjzXdt4pQpIkqecMdJIkST1noJMkSeo5A50kSVLPGegkSZJ6zkAnSZLUcwY6SZKknhvZhYXXZosOOmnSJaxWlxy2x6RLkCRJs3CETpIkqecMdJIkST1noJMkSeo5A50kSVLPGegkSZJ6zkAnSZLUcwY6SZKknjPQSZIk9ZyBTpIkqecMdJIkST1noJMkSeo5A50kSVLPGegkSZJ6zkAnSZLUcwY6SZKknjPQSZIk9ZyBTpIkqecMdJIkST1noJMkSeo5A50kSVLPGegkSZJ6zkAnSZLUcwY6SZKknjPQSZIk9ZyBTpIkqecMdJIkST1noJMkSeo5A50kSVLPGegkSZJ6zkAnSZLUcwY6SZKknjPQSZIk9ZyBTpIkqecMdJIkST1noJMkSeo5A50kSVLPGegkSZJ6zkAnSZLUcwY6SZKknjPQSZIk9ZyBTpIkqecMdJIkST1noJMkSeo5A50kSVLPGegkSZJ6zkAnSZLUcwY6SZKknjPQSZIk9dxEAl2SVyY5P8l5ST6V5F5JtktyWpIlST6d5J5t2/Xb8pK2ftHA67yhtf8oyW6T+CySJEmTNvZAl2RL4OXA4qr6A2AdYG/gncD7qmoH4Fpg/7bL/sC1rf19bTuS7NT2eyjwNOCDSdYZ52eRJEmaDyZ1yHVd4N5J1gU2AK4Angwc19YfBezVnu/Zlmnrd02S1n5sVd1WVT8FlgA7j6l+SZKkeWPsga6qLgfeDfyMLshdD3wfuK6q7mibLQW2bM+3BC5r+97Rtt9ksH2afe4myQFJzkhyxrJly1bvB5IkSZqwSRxy3YhudG07YAvgPnSHTEemqo6oqsVVtXjhwoWjfCtJkqSxm8Qh16cAP62qZVX1a+DzwOOABe0QLMBWwOXt+eXA1gBt/f2BXw62T7OPJEnSWmMSge5nwC5JNmjnwu0KXAD8F/Csts2+wBfb8xPaMm39N6qqWvvebRbsdsCOwPfG9BkkSZLmjXXn3mT1qqrTkhwHnAncAZwFHAGcBByb5B9b20faLh8BjkmyBLiGbmYrVXV+ks/QhcE7gAOr6s6xfhhJkqR5YOyBDqCqDgYOXq75YqaZpVpVtwLPnuF1DgUOXe0FSpIk9Yh3ipAkSeo5A50kSVLPGegkSZJ6zkAnSZLUcwY6SZKknpvILFet+RYddNKkS1itLjlsj0mXIEnSjByhkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPTdnoEuyfZL12/MnJnl5kgWjL02SJEnDGGaE7nPAnUl2AI4AtgY+OdKqJEmSNLRhAt1dVXUH8H+Bf66q1wKbj7YsSZIkDWuYQPfrJM8D9gVObG3rja4kSZIkrYhhAt1+wGOBQ6vqp0m2A44ZbVmSJEka1rpzbVBVFyR5PbBNW/4p8M5RFyZJkqThDDPL9RnA2cBX2vIjk5ww6sIkSZI0nGEOub4F2Bm4DqCqzgYeOMKaJEmStAKGmhRRVdcv13bXKIqRJEnSipvzHDrg/CTPB9ZJsiPwcuB/R1uWJEmShjXMCN3LgIcCtwGfAm4A/m6URUmSJGl4w8xyvQX4e+Dvk6wD3Keqbh15ZZIkSRrKMLNcP5lkwyT3AX4AXJDktaMvTZIkScMY5pDrTlV1A7AX8GVgO+BFI61KkiRJQxsm0K2XZD26QHdCVf0aqNGWJUmSpGENE+j+HbgEuA/w7STb0k2MkCRJ0jwwzKSIw4HDB5ouTfKk0ZUkSZKkFTHMCB1J9kjyuiRvTvJm4I2r8qZJFiQ5LskPk1yY5LFJNk5ySpKL2s+N2rZJcniSJUnOTfKogdfZt21/UZJ9V6UmSZKkvhpmluu/Ac+lux5dgGcD267i+34A+EpVPRh4BHAhcBDw9araEfh6WwZ4OrBjexwAfKjVtTFwMPAYuluTHTwVAiVJktYmw4zQ/XFV7QNcW1VvBR4LPGhl3zDJ/YHHAx8BqKrbq+o6YE/gqLbZUXSTMGjtR1fnVGBBks2B3YBTquqaqroWOAV42srWJUmS1FfDBLpftZ+3JNkC+DWw+Sq853bAMuCjSc5K8uF2jbvNquqKts2VwGbt+ZbAZQP7L21tM7VLkiStVYa5l+uJSRYA7wLOpLtkyYdX8T0fBbysqk5L8gF+e3gVgKqqJKvt0ihJDqA7XMs222yzul5WmtWig06adAmr1SWH7THpEiRJM5hzhK6q3lZV11XV5+jOnXtwVb1pFd5zKbC0qk5ry8fRBbyr2qFU2s+r2/rLga0H9t+qtc3UPt1nOKKqFlfV4oULF65C6ZIkSfPPsLNc/zjJ8+kmR+yZZJ+VfcOquhK4LMnvt6ZdgQuAE4Cpmar7Al9sz08A9mmzXXcBrm+HZr8KPDXJRm0yxFNbmyRJ0lplzkOuSY4BtgfOBu5szQUcvQrv+zLgE0nuCVwM7EcXLj+TZH/gUuA5bduTgd2BJcAtbVuq6pokbwNOb9sdUlXXrEJNkiRJvTTMOXSL6e7nutrOaauqs9vrLm/XabYt4MAZXudI4MjVVZckSVIfDXPI9TzgAaMuRJIkSStnxhG6JF+iO7R6P+CCJN8DbptaX1XPHH15kiRJmstsh1zfPbYqJEmStNJmDHRV9S2AJNsBV1TVrW353vz2or+SJEmasGHOofsscNfA8p2tTZIkSfPAMIFu3aq6fWqhPb/n6EqSJEnSihgm0C1L8psJEEn2BH4xupIkSZK0Ioa5Dt3f0F0E+F/a8lLgRaMrSZIkSSti1kCX5B7AH1XVLknuC1BVN42lMkmSJA1l1kOuVXUX8Lr2/CbDnCRJ0vwzzDl0X0vymiRbJ9l46jHyyiRJkjSUYc6he277OXg/1QIeuPrLkSRJ0oqaM9BV1XbjKESSJEkrZ85Al2Sf6dqr6ujVX44kSZJW1DCHXB898PxewK7AmYCBTpIkaR4Y5pDrywaXkywAjh1ZRZIkSVohw8xyXd7NgOfVSZIkzRPDnEP3JbpZrdAFwJ2Az4yyKEmSJA1vmHPo3j3w/A7g0qpaOqJ6JEmStILmuvXXXsAOwA+q6qvjKUmSJEkrYsZz6JJ8EHglsAnwtiRvGltVkiRJGtpsI3SPBx5RVXcm2QD4DvC28ZQlSZKkYc02y/X2qroToKpuATKekiRJkrQiZhuhe3CSc9vzANu35QBVVQ8feXWSJEma02yB7iFjq0KSJEkrbcZAV1WXjrMQSZIkrZyVuVOEJEmS5hEDnSRJUs/Ndh26r7ef7xxfOZIkSVpRs02K2DzJHwPPTHIsy122pKrOHGllkiRJGspsge7NwJuArYD3LreugCePqihJkiQNb7ZZrscBxyV5U1V5hwhJkqR5arYROgCq6m1Jnkl3KzCAb1bViaMtS5IkScOac5ZrkncArwAuaI9XJHn7qAuTJEnScOYcoQP2AB5ZVXcBJDkKOAt44ygLkyRJ0nCGvQ7dgoHn9x9FIZIkSVo5w4zQvQM4K8l/0V265PHAQSOtSpIkSUMbZlLEp5J8E3h0a3p9VV050qokSZI0tGFG6KiqK4ATRlyLJEmSVoL3cpUkSeo5A50kSVLPzRrokqyT5IfjKkaSJEkrbtZAV1V3Aj9Kss2Y6pEkSdIKGmZSxEbA+Um+B9w81VhVzxxZVZIkSRraMIHuTSOvQpIkSSttmOvQfSvJtsCOVfW1JBsA64y+NEmSJA1jzlmuSf4KOA7499a0JXD8KIuSJEnS8Ia5bMmBwOOAGwCq6iLg90ZZlCRJkoY3zDl0t1XV7UkASLIuUCOtSpKkNciig06adAmr1SWH7THpErScYUbovpXkjcC9k/wp8FngS6MtS5IkScMaZoTuIGB/4AfAXwMnAx8eZVGSpDWHo1PS6A0zy/WuJEcBp9Edav1RVXnIVZIkaZ6YM9Al2QP4N+AnQIDtkvx1VX151MVJkiRpbsMccn0P8KSqWgKQZHvgJMBAJ0mSNA8ME+hunApzzcXAjSOqR9IaxHOnJGk8Zgx0Sf68PT0jycnAZ+jOoXs2cPoYapMkSdIQZrtsyTPa417AVcATgCcCy4B7r+obJ1knyVlJTmzL2yU5LcmSJJ9Ocs/Wvn5bXtLWLxp4jTe09h8l2W1Va5IkSeqjGUfoqmq/Eb/3K4ALgQ3b8juB91XVsUn+je5SKR9qP6+tqh2S7N22e26SnYC9gYcCWwBfS/KgqrpzxHVLkiTNK8Pcy3W7JO9N8vkkJ0w9VuVNk2wF7EG7nl2621A8me6esQBHAXu153u2Zdr6Xdv2ewLHVtVtVfVTYAmw86rUJUmS1EfDTIo4HvgI3d0h7lpN7/t+4HXA/dryJsB1VXVHW14KbNmebwlcBlBVdyS5vm2/JXDqwGsO7nM3SQ4ADgDYZpttVtNHkCRJw3KS1GgNE+hurarDV9cbJvkz4Oqq+n6SJ66u151NVR0BHAGwePFiL4osSZLWKMMEug8kORj4T+C2qcaqOnMl3/NxwDOT7E434WJD4APAgiTrtlG6rYDL2/aXA1sDS5OsC9wf+OVA+5TBfSRJktYawwS6hwEvojvHbeqQa7XlFVZVbwDeANBG6F5TVS9I8lngWcCxwL7AF9suJ7Tl77b136iqaufxfTLJe+kmRewIfG9lapKkUfEwk6RxGCbQPRt4YFXdPuJaXg8cm+QfgbPoztuj/TwmyRLgGrqZrVTV+Uk+A1wA3AEc6AxXSZK0Nhom0J0HLACuXt1vXlXfBL7Znl/MNLNUq+pWulA53f6HAoeu7rokSZL6ZJhAtwD4YZLTufs5dM8cWVWSJEka2jCB7uCRVyFJkqSVNmegq6pvjaMQSZIkrZw5A12SG+lmtQLcE1gPuLmqNpx5L0mSJI3LMCN0U3dzYOCWW7uMsihJkiQNb857uQ6qzvHAbiOqR5IkSStomEOufz6weA9gMXDryCqSJEnSChlmluszBp7fAVxCd9hVkiRJ88Aw59DtN45CJEmStHJmDHRJ3jzLflVVbxtBPZIkSVpBs43Q3TxN232A/YFNAAOdJEnSPDBjoKuq90w9T3I/4BXAfsCxwHtm2k+SJEnjNes5dEk2Bl4FvAA4CnhUVV07jsIkSZI0nNnOoXsX8OfAEcDDquqmsVUlSZKkoc12YeFXA1sA/wD8PMkN7XFjkhvGU54kSZLmMts5dCt0FwlJkiRNhqFNkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT13NgDXZKtk/xXkguSnJ/kFa194ySnJLmo/dyotSfJ4UmWJDk3yaMGXmvftv1FSfYd92eRJEmaDyYxQncH8Oqq2gnYBTgwyU7AQcDXq2pH4OttGeDpwI7tcQDwIegCIHAw8BhgZ+DgqRAoSZK0Nhl7oKuqK6rqzPb8RuBCYEtgT+CottlRwF7t+Z7A0dU5FViQZHNgN+CUqrqmqq4FTgGeNsaPIkmSNC9M9By6JIuAPwROAzarqivaqiuBzdrzLYHLBnZb2tpmap/ufQ5IckaSM5YtW7ba6pckSZoPJhboktwX+Bzwd1V1w+C6qiqgVtd7VdURVbW4qhYvXLhwdb2sJEnSvDCRQJdkPbow94mq+nxrvqodSqX9vLq1Xw5sPbD7Vq1tpnZJkqS1yiRmuQb4CHBhVb13YNUJwNRM1X2BLw6079Nmu+4CXN8OzX4VeGqSjdpkiKe2NkmSpLXKuhN4z8cBLwJ+kOTs1vZG4DDgM0n2By4FntPWnQzsDiwBbgH2A6iqa5K8DTi9bXdIVV0zno8gSZI0f4w90FXVfwOZYfWu02xfwIEzvNaRwJGrrzpJkqT+8U4RkiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJkqSeM9BJkiT1XO8DXZKnJflRkiVJDpp0PZIkSePW60CXZB3gX4GnAzsBz0uy02SrkiRJGq9eBzpgZ2BJVV1cVbcDxwJ7TrgmSZKksep7oNsSuGxgeWlrkyRJWmukqiYYn95zAAAGNklEQVRdw0pL8izgaVX14rb8IuAxVfXS5bY7ADigLf4+8KOxFjo6mwK/mHQRE2Yf2AdgH4B9APYB2Aew5vXBtlW1cK6N1h1HJSN0ObD1wPJWre1uquoI4IhxFTUuSc6oqsWTrmOS7AP7AOwDsA/APgD7ANbePuj7IdfTgR2TbJfknsDewAkTrkmSJGmsej1CV1V3JHkp8FVgHeDIqjp/wmVJkiSNVa8DHUBVnQycPOk6JmSNO4y8EuwD+wDsA7APwD4A+wDW0j7o9aQISZIk9f8cOkmSpLWega4nprvFWZKXtuVKsumkaxy1GfrgE63tvCRHJllv0nWO0gx98JEk5yQ5N8lxSe476TpHabbb/SU5PMlNk6ptXGb4HnwsyU+TnN0ej5x0naM0Qx8kyaFJfpzkwiQvn3SdozRDH3xn4Dvw8yTHT7rOUZuhH3ZNcmbrh/9OssOk6xy5qvIxzx90Ez5+AjwQuCdwDt2tzv4QWARcAmw66Ton1Ae7A2mPTwF/O+laJ9AHGw5s817goEnXOu4+aOsWA8cAN026zgl9Dz4GPGvS9U24D/YDjgbu0bb7vUnXOu4+WG6bzwH7TLrWCX0Xfgw8pG3zEuBjk6511A9H6Pph2lucVdVZVXXJZEsbm5n64ORqgO/RXYtwTTVTH9wA3egEcG9gTT4xdto+aPd1fhfwuolWNx7e8nDmPvhb4JCqugugqq6eYI2jNuv3IMmGwJOBNX2EbqZ+KGDDts39gZ9PqL6xMdD1g7c4m6MP2qHWFwFfGXNd4zRjHyT5KHAl8GDgn8df2tjM1AcvBU6oqismUtV4zfbfwqHt0Pv7kqw//tLGZqY+2B54bpIzknw5yY4TqW485vq7sBfw9an/4VuDzdQPLwZOTrKU7m/DYROobawMdFpTfBD4dlV9Z9KFTEJV7QdsAVwIPHfC5YzbBsCzWbOD7DDeQBfoHw1sDLx+suVMxPrArdXdJeA/gCMnXM8kPY/uNJS11SuB3atqK+CjdKejrNEMdP0w1C3O1nAz9kGSg4GFwKsmUNc4zfo9qKo76Q43/MWY6xqn6frgJ8AOwJIklwAbJFkygdrGZdrvQVVd0c4+uI3uD9jOE6luPGb6b2Ep8PnW9gXg4WOua5xm+zdxU7rf/0kTqGvcpuuHq4BHVNVpre3TwB+Pu7BxM9D1g7c4m6EPkrwY2A143tR5M2uwmfpgB/jNOXTPBH44wRpHbbo+OL6qHlBVi6pqEXBLVa3JM9pm+h5sDr/5HuwFnDfBGkdtpn8Tjwee1LZ5At2J8Wuq2f4uPAs4sapunVh14zNTP9w/yYPaNn9Kd/Rijdb7O0WsDWqGW5y1KfmvAx4AnJvk5Kp68SRrHZVZ+uAc4FLgu93fMT5fVYdMsNSRma4P6P6R+k47ATp0M7z+dnJVjtZM34MJlzVWs/y38I0kC+m+B2cDfzPJOkdplj44DPhEklcCN9GdR7VGmuO/hb1ZC84Zgxn74ZwkfwV8LsldwLXA/5tknePgnSIkSZJ6zkOukiRJPWegkyRJ6jkDnSRJUs8Z6CRJknrOQCdJktRzBjpJapJUko8PLK+bZFmSE1fy9RYkecnA8hNX9rUkaTYGOkn6rZuBP0hy77b8p6zaXVkWAC+ZcytJWkUGOkm6u5OBPdrzu90PM8nGSY5Pcm6SU5M8vLW/JcmRSb6Z5OJ20W/oLu66fZKzk7yrtd03yXFJfpjkE+3ODpK0Sgx0knR3xwJ7J7kX3b1ATxtY91bgrKp6OPBG4OiBdQ+muw3dzsDBSdYDDgJ+UlWPrKrXtu3+EPg7YCfggcDjRvlhJK0dDHSSNKCqzgUW0Y3Onbzc6j8BjmnbfQPYpN12DeCkqrqtqn4BXA1sNsNbfK+qlrZ7D5/d3kuSVon3cpWk33UC8G7gicAmQ+5z28DzO5n539dht5OkoTlCJ0m/60jgrVX1g+XavwO8ALoZq8AvquqGWV7nRuB+I6lQkgb4f4aStJyqWgocPs2qtwBHJjkXuAXYd47X+WWS/0lyHvBl4KTVXaskAaSqJl2DJEmSVoGHXCVJknrOQCdJktRzBjpJkqSeM9BJkiT1nIFOkiSp5wx0kiRJPWegkyRJ6jkDnSRJUs/9f/twZHYqJj8NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_purchase_counts(df, \"Baby\", 2015)\n",
    "plot_purchase_counts(df, \"Baby\", 1999)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "190px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "514px",
    "left": "0px",
    "right": "925px",
    "top": "107px",
    "width": "323px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
