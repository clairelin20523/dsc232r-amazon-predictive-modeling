{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/55094320/how-to-read-a-large-tsv-file-in-python-and-convert-it-to-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType,BinaryType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config('spark.executor.instances', 3) \\\n",
    "    .appName(\"Amazon Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sqlContext = SQLContext(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"amazon_reviews_us_Baby_v1_00.tsv\", sep = \"\\t\", header = True, inferSchema = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(df, \"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total # of variables \n",
    "num_cols = len(df.columns)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total # of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select(df.columns[:8]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select(df.columns[8:]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing repetitive/unnecssary information\n",
    "df = df.select([col for col in df.columns if col not in ['marketplace', 'vine']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Check for null values in all columns\n",
    "null_counts = df.select([col(c).isNull().cast('int').alias(c) for c in df.columns])\n",
    "\n",
    "# Sum up the counts of null values in each column\n",
    "total_null_counts = null_counts.agg(*[F.sum(c).alias(c) for c in null_counts.columns])\n",
    "\n",
    "# Display the total counts of null values in each column\n",
    "total_null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('review_date').count().show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT\n",
    "    SUBSTRING(review_date, 1, 4) AS year,\n",
    "    COUNT(*) AS count\n",
    "FROM\n",
    "    df\n",
    "GROUP BY\n",
    "    SUBSTRING(review_date, 1, 4)\n",
    "ORDER BY\n",
    "    year ASC\n",
    "\"\"\"\n",
    "result_df = spark.sql(sql_query)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from lib.YearPlotter import YearPlotter\n",
    "# k=5\n",
    "# _title='TOBS for %s / %d'%(station,year)\n",
    "# fig, ax = plt.subplots(figsize=_figsize);\n",
    "# YP=YearPlotter()\n",
    "# YP.plot(M[:k,:366].T,fig,ax,title=_title,labels=_labels)# ,labels=labels);\n",
    "# ylabel('temp in centigrade');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM df\n",
    "    WHERE YEAR(review_date) = 2015\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result_df = spark.sql(sql_query)\n",
    "\n",
    "# Show the result DataFrame\n",
    "result_df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the SQL query\n",
    "result_df = spark.sql(sql_query)\n",
    "\n",
    "# Count the number of rows in the result DataFrame\n",
    "result_count = result_df.count()\n",
    "\n",
    "# Show the result count\n",
    "print(\"Count of review dates in 2015:\", result_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_star_rating_distribution(df,df_name):\n",
    "    \"\"\"\n",
    "    Plot the distribution of star ratings for a given DataFrame, given that each Amazon category is a a different df. \n",
    "    ex. baby = df, toy= df1\n",
    "\n",
    "    Args:\n",
    "    - df: The DataFrame containing the star ratings.\n",
    "    - df_name: The name of the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Count the number of occurrences for each star rating\n",
    "    star_rating_counts = df.groupBy('star_rating').count().orderBy('star_rating')\n",
    "\n",
    "    # Plot the counts using a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(star_rating_counts.toPandas()['star_rating'], star_rating_counts.toPandas()['count'])\n",
    "    plt.title(f'Star Rating Distribution {df_name}')\n",
    "    plt.xlabel('Star Rating')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_star_rating_distribution(df,'Baby')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, substring\n",
    "\n",
    "def plot_purchase_counts(df, df_name, target_year):\n",
    "    \"\"\"\n",
    "    Plot the purchase counts for a given DataFrame and year.\n",
    "\n",
    "    Args:\n",
    "    - df: The DataFrame containing the review data.\n",
    "    - target_year: The year for which purchase counts will be plotted.\n",
    "    - df_name: The name of the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include only rows from the specified year\n",
    "    df_year = df.filter(year(df['review_date']) == target_year)\n",
    "\n",
    "    # Extract the month from the 'review_date' column\n",
    "    df_year = df_year.withColumn('review_month', substring(df_year['review_date'], 6, 2))\n",
    "\n",
    "    # Count the number of purchases for each month\n",
    "    purchase_counts = df_year.groupby('review_month').count().orderBy('review_month')\n",
    "\n",
    "    # Plot the counts using a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(purchase_counts.toPandas()['review_month'], purchase_counts.toPandas()['count'])\n",
    "    plt.title(f'Purchase Counts for {df_name} Year {target_year}')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Purchases')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_purchase_counts(df, \"Baby\", 2015)\n",
    "plot_purchase_counts(df, \"Baby\", 1999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
