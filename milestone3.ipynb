{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Amazon Review Data : Data Exploration\n",
    "\n",
    "### Setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-2vvhgvya because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os, pickle, glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.ml.feature import Word2Vec, Tokenizer, StringIndexer, OneHotEncoder, PCA, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "\t.config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config('spark.executor.instances', 5) \\\n",
    "\t.appName(\"Amazon Reviews\") \\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Read Data\n",
    "\n",
    "### Get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "###################### change path to: \"../clin6/amazon_data\" #########################\n",
    "#######################################################################################\n",
    "path = \"../clin6/amazon_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    \"\"\"\n",
    "    Method that loads data file as df\n",
    "    Takes in 1 parameter: path\n",
    "    \"\"\"\n",
    "    return sc.read.csv(path, sep = \"\\t\", header = True, inferSchema = True)\n",
    "\n",
    "def get_path(file):\n",
    "    \"\"\"\n",
    "    Method to create path\n",
    "    Takes in 1 parameter: file name\n",
    "    \"\"\"\n",
    "    return \"amazon_data/%s\" % file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.listdir(path)\n",
    "files = [f for f in dir if os.path.isfile(os.path.join(path, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_df(files):\n",
    "    \"\"\"\n",
    "    Method that combines files into 1 big df\n",
    "    Takes in 1 parameter: list of file names\n",
    "    \"\"\"\n",
    "    df = None\n",
    "    n = len(files)\n",
    "    for i in range(n):\n",
    "        data = read(get_path(files[i]))\n",
    "        if df is None:\n",
    "            df = data\n",
    "        else:\n",
    "            df = df.union(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "* Finish major preprocessing, this includes scaling and/or transforming your data, imputing your data, encoding your data, feature expansion, Feature expansion (example is taking features and generating new features by transforming via polynomial, log multiplication of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_category_column = 'product_category'\n",
    "review_body_column = 'review_body'\n",
    "review_date_column = 'review_date'\n",
    "title_column = 'product_title'\n",
    "category_column = 'product_category'\n",
    "product_parent_column = 'product_parent'\n",
    "review_body_column = 'review_body'\n",
    "verified_purchase_column = 'verified_purchase'\n",
    "\n",
    "product_category_col = F.col(product_category_column)\n",
    "review_body_col = F.col(review_body_column)\n",
    "review_date_col = F.col(review_date_column)\n",
    "title_col = F.col(title_column)\n",
    "category_col = F.col(category_column)\n",
    "product_parent_col = F.col(product_parent_column)\n",
    "review_body_col = F.col(review_body_column)\n",
    "verified_purchase_col = F.col(verified_purchase_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Take care of missing categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputed_df(files, category = True):\n",
    "    \"\"\"\n",
    "    Method that combines files into 1 big df\n",
    "    Takes in 1 parameter: list of file names\n",
    "    \"\"\"\n",
    "    df = None\n",
    "    n = len(files)\n",
    "    categories = {}\n",
    "    for i in range(n):\n",
    "        data = read(get_path(files[i]))\n",
    "        \n",
    "        # Fill in null categories\n",
    "        if category:\n",
    "            cat = files[i][18:-10]\n",
    "            categories[cat] = i\n",
    "            data = data.withColumn(product_category_column,\n",
    "                                   product_category_col).fillna(cat)\n",
    "        \n",
    "        if df is None:\n",
    "            df = data\n",
    "        else:\n",
    "            df = df.union(data)\n",
    "    return df, categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get df & Remove Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, categories = get_imputed_df(files)\n",
    "df = df.drop('marketplace', 'vine').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(df, \"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "num_cols = len(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out rows with missing body and date and verified purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(review_body_col.isNotNull() & review_date_col.isNotNull())\n",
    "df = df.filter(verified_purchase_col == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(F.year(review_date_col) >= 2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check other missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'customer_id' column has 0 missing values\n",
      "'review_id' column has 0 missing values\n",
      "'product_id' column has 0 missing values\n",
      "'product_parent' column has 0 missing values\n",
      "'product_title' column has 0 missing values\n",
      "'product_category' column has 0 missing values\n",
      "'star_rating' column has 0 missing values\n",
      "'helpful_votes' column has 0 missing values\n",
      "'total_votes' column has 0 missing values\n",
      "'verified_purchase' column has 0 missing values\n",
      "'review_headline' column has 0 missing values\n",
      "'review_body' column has 0 missing values\n",
      "'review_date' column has 0 missing values\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_cols):\n",
    "    missing = df.filter(df[columns[i]].isNull()).count()\n",
    "    print(\"'%s' column has %d missing values\" % (columns[i], missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No more missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_column = 'month'\n",
    "year_column = 'year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(month_column, F.month(review_date_col)).withColumn(year_column, F.year(review_date_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match category to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_num_col = 'product_category_num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(dic): \n",
    "    return F.udf(lambda x: dic.get(x), StringType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(category_num_col, translate(categories)(category_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change title into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleArray_column = 'titleArray'\n",
    "titleVector_column = 'titleVector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(titleArray_column, F.split(F.lower(F.col(title_column)), ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(inputCol = titleArray_column, outputCol = titleVector_column,\n",
    "                    minCount = 100, vectorSize = 16, numPartitions = 4)\n",
    "model = word2vec.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change text into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewArray_column = 'reviewArray'\n",
    "reviewVector_column = 'reviewVector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(reviewArray_column, F.split(F.lower(F.col(review_body_column)), ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(inputCol = reviewArray_column, outputCol = reviewVector_column,\n",
    "                    minCount = 100, vectorSize = 16, numPartitions = 4)\n",
    "model = word2vec.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "* Use last year as test and the rest as train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.filter(F.year(review_date_col) < 2015)\n",
    "test = df.filter(F.year(review_date_col) >= 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58754087\n",
      "27498612\n"
     ]
    }
   ],
   "source": [
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Product Reviews Per Day\n",
    "* Group by unique product identifier and day to get reviews per day for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(review_date_col, product_parent_col).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder size: 13204063462 bytes\n",
      "Folder size: 12894593.224609375 KB\n",
      "Folder size: 12592.376195907593 MB\n",
      "Folder size: 12.297242378816009 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "folder_path = \"../clin6/data\"\n",
    "folder_size_bytes = get_folder_size(folder_path)\n",
    "folder_size_kb = folder_size_bytes / 1024\n",
    "folder_size_mb = folder_size_kb / 1024\n",
    "folder_size_gb = folder_size_mb / 1024\n",
    "\n",
    "print(\"Folder size:\", folder_size_bytes, \"bytes\")\n",
    "print(\"Folder size:\", folder_size_kb, \"KB\")\n",
    "print(\"Folder size:\", folder_size_mb, \"MB\")\n",
    "print(\"Folder size:\", folder_size_gb, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data modeling\n",
    "* Train your first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Evaluation\n",
    "* Evaluate your model and compare training vs. test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer the questions\n",
    "* Where does your model fit in the fitting graph? and What are the next models you are thinking of and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion section\n",
    "* What is the conclusion of your 1st model? What can be done to possibly improve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "190px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "514px",
    "left": "0px",
    "right": "925px",
    "top": "107px",
    "width": "323px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
